{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g5GN3EpDn4b"
      },
      "source": [
        "\n",
        "Imports e definição do dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gpxhsZya71hk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zVrlrW_OXs-1"
      },
      "outputs": [],
      "source": [
        "# Carregar o conjunto de dados CIFAR-10\n",
        "#Carrega duas tuplas, representando os dados de treinamento e de teste.\n",
        "#Cada tupla tem as imagens e os respectivos rótulos\n",
        "datasets = {\n",
        "    'cifar10'       : [keras.datasets.cifar10,       (32, 32, 3),  10],\n",
        "    'cifar100'      : [keras.datasets.cifar100,      (32, 32, 3), 100],\n",
        "    'mnist'         : [keras.datasets.mnist,         (28, 28, 1),  10],\n",
        "    'fashion_mnist' : [keras.datasets.fashion_mnist, (28, 28, 1),  10]\n",
        "}\n",
        "\n",
        "dataset = datasets['fashion_mnist']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nymnOpRMETAu"
      },
      "source": [
        "Montar as redes que vão ser usadas para testar.\n",
        "\n",
        "Todas as redes compartilham a mesma \"head\" densa, uma camada  com 64 neurônios e ativação ReLU e uma camada de saída com n (onde n é o número de classes do dataset) neurônios e ativação softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kxT8z7eJepKg"
      },
      "outputs": [],
      "source": [
        "# Layer Residual (Implementação baseada em: https://gist.github.com/FirefoxMetzger/6b6ccf4f7c344459507e73bbd13ec541)\n",
        "class Residual(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels_in,kernel, strides=1, **kwargs):\n",
        "        super(Residual, self).__init__(**kwargs)\n",
        "        self.channels_in = channels_in\n",
        "        self.kernel = kernel\n",
        "        self.conv1 = tf.keras.layers.Conv2D(self.channels_in,\n",
        "                                self.kernel,\n",
        "                                padding = 'same',\n",
        "                                strides=strides,\n",
        "                                activation='relu')\n",
        "        self.batch1 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.conv2 = tf.keras.layers.Conv2D(self.channels_in,\n",
        "                                self.kernel,\n",
        "                                padding = 'same',\n",
        "                                strides=strides,\n",
        "                                activation='relu')\n",
        "\n",
        "        self.batch2 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv3 = tf.keras.layers.Conv2D(self.channels_in,\n",
        "                                self.kernel,\n",
        "                                padding = 'same',\n",
        "                                strides=strides,\n",
        "                                activation='relu')\n",
        "        self.batch3 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.conv4 = tf.keras.layers.Conv2D(self.channels_in,\n",
        "                                self.kernel,\n",
        "                                padding = 'same',\n",
        "                                strides=strides,\n",
        "                                activation='relu')\n",
        "\n",
        "    def call(self, x):\n",
        "        # the residual block using Keras functional API\n",
        "        input_layer =   tf.keras.layers.Activation('linear')(x)\n",
        "        x =             self.conv1(input_layer)\n",
        "        x =             self.batch1(x)\n",
        "        x =             tf.keras.layers.Dropout(0.25)(x)\n",
        "        x =             self.conv2(x)\n",
        "        x =             self.batch2(x)\n",
        "        x =             tf.keras.layers.Dropout(0.25)(x)\n",
        "        x =             self.conv3(x)\n",
        "        x =             self.batch3(x)\n",
        "        x =             self.conv3(x)\n",
        "        residual =      tf.keras.layers.Add()([x, input_layer])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "nPrIRBmT8XiN"
      },
      "outputs": [],
      "source": [
        "# Crie o modelo de rede neural convolucional\n",
        "input_shape = dataset[1]\n",
        "def get_advanced_network():\n",
        "  model =  keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(64, (2, 2), strides=2, activation='relu', input_shape=input_shape),\n",
        "      Residual(64, (7,7), strides=1),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "      tf.keras.layers.Conv2D(128, (5, 5), strides=1, activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "      tf.keras.layers.Conv2D(128, (3, 3), strides=1, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.GlobalAveragePooling2D()\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def compose_with_simple_head(backbone, num_classes):\n",
        "    model = tf.keras.models.clone_model(backbone)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu')),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu')),\n",
        "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile o modelo\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',#pode ser substituída pela esparse_categorical_cross_entropy\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "def compose_with_advanced_head(backbone, num_classes):\n",
        "    model = tf.keras.models.clone_model(backbone)\n",
        "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile o modelo\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',#pode ser substituída pela esparse_categorical_cross_entropy\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjmscwcQErIx"
      },
      "source": [
        "Trecho para treinar e avaliar a rede neural.\n",
        "O treino é realizado com os dados de treino e a avaliação do modelo é realizada nos dados de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uN8v8_m8cvR",
        "outputId": "2becc3c9-8a06-42a8-b951-18a307e50309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_345 (Conv2D)         (None, 14, 14, 64)        320       \n",
            "                                                                 \n",
            " residual_37 (Residual)      (None, 14, 14, 64)        603072    \n",
            "                                                                 \n",
            " batch_normalization_282 (B  (None, 14, 14, 64)        256       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_59 (Dropout)        (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_350 (Conv2D)         (None, 10, 10, 128)       204928    \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPoolin  (None, 5, 5, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_283 (B  (None, 5, 5, 128)         512       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_60 (Dropout)        (None, 5, 5, 128)         0         \n",
            "                                                                 \n",
            " conv2d_351 (Conv2D)         (None, 3, 3, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_284 (B  (None, 3, 3, 128)         512       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " global_average_pooling2d_2  (None, 128)               0         \n",
            " 2 (GlobalAveragePooling2D)                                      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 958474 (3.66 MB)\n",
            "Trainable params: 957450 (3.65 MB)\n",
            "Non-trainable params: 1024 (4.00 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 15s 6ms/step - loss: 0.4783 - accuracy: 0.8219\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3288 - accuracy: 0.8781\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2902 - accuracy: 0.8928\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2620 - accuracy: 0.9036\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2392 - accuracy: 0.9099\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2226 - accuracy: 0.9180\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2077 - accuracy: 0.9232\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1948 - accuracy: 0.9276\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1817 - accuracy: 0.9323\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1678 - accuracy: 0.9375\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2405 - accuracy: 0.9168\n",
            "Acurácia no conjunto de teste: 91.68%\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = dataset[0].load_data()\n",
        "num_classes = dataset[2]\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = keras.utils.to_categorical(test_labels, num_classes)\n",
        "\n",
        "# Treine o modelo\n",
        "model = compose_with_advanced_head(get_advanced_network(), num_classes)\n",
        "model.fit(train_images, train_labels, epochs=10)\n",
        "\n",
        "# Avalie o modelo no conjunto de teste\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f'Acurácia no conjunto de teste: {test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc5-m031HdY0"
      },
      "source": [
        "Na célula abaixo, adicione o código para carregar os demais datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKkBlEy1ExHj"
      },
      "source": [
        "Tarefa\n",
        "Escreva código para executar redes neurais nos seguintes datasets:\n",
        "\n",
        "MNIST (pode aproveitar o codigo existente)\n",
        "Fashion MNIST\n",
        "CIFAR-10\n",
        "CIFAR-100\n",
        "Cada execução deve ser por 10 épocas.\n",
        "\n",
        "Você deve preencher as funções a seguir para retornarem a rede neural com a melhor configuração que você conseguiu para cada dataset. O notebook deve ser entregue com a rede neural que obteve a melhor performance em cada conjunto de dados.\n",
        "\n",
        "IMPORTANTE: as funções não devem TREINAR nem AVALIAR as redes neurais, apenas instanciá-las e retorná-las.\n",
        "\n",
        "Ao final, preencha o dict results com o desempenho encontrado em cada execução."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN14IpXnFfCQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_fashion_mnist_backbone():\n",
        "  model =  keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(64, (2, 2), strides=2, activation='relu', input_shape=datasets['fashion_mnist'][1]),\n",
        "      Residual(64, (7,7), strides=1),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "      tf.keras.layers.Conv2D(128, (5, 5), strides=1, activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "      tf.keras.layers.Conv2D(128, (3, 3), strides=1, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.GlobalAveragePooling2D()\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def get_fashion_mnist_network():\n",
        "  return compose_with_simple_head(get_fashion_mnist_backbone(), datasets['fashion_mniist'][2])\n",
        "\n",
        "def get_mnist_backbone():\n",
        "  model =  keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (2, 2), strides=2, activation='relu', input_shape=datasets['mnist'][1]),\n",
        "    Residual(32, (7,7)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Conv2D(64, (5, 5), strides=1, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), strides=1, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    Residual(128, (3,3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Conv2D(256, (2, 2), strides=1, activation='relu'),\n",
        "    tf.keras.layers.GlobalAveragePooling2D()\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def get_mnist_network():\n",
        "  return compose_with_simple_head(get_mnist_backbone(), datasets['mnist'][2])\n",
        "\n",
        "def get_cifar100_backbone():\n",
        "  model =  keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(64, (2, 2), strides=1, activation='relu', input_shape=input_shape),\n",
        "      Residual(64, (7,7), strides=1),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "      tf.keras.layers.Conv2D(128, (5, 5), strides=1, activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "      tf.keras.layers.Conv2D(128, (3, 3), strides=1, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.GlobalAveragePooling2D()\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def get_cifar100_network():\n",
        "    return compose_with_simple_head(get_cifar100_backbone(), datasets['cifar100'][2])\n",
        "\n",
        "def get_cifar10_backbone():\n",
        "  model =  keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(128, (2, 2), strides=2, activation='relu', input_shape=datasets['cifar10'][1]),\n",
        "      Residual(128, (7,7)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "      tf.keras.layers.Conv2D(64, (5, 5), strides=1, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "      tf.keras.layers.Conv2D(128, (3, 3), strides=1, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "      Residual(128, (3,3)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "      tf.keras.layers.Conv2D(256, (2, 2), strides=1, activation='relu'),\n",
        "      tf.keras.layers.GlobalAveragePooling2D()\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def get_cifar10_network():\n",
        "  return compose_with_simple_head(get_cifar10_backbone(), datasets['cifar10'][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iERVafMPF2Tn"
      },
      "source": [
        "Preencha o dict abaixo substituindo os None com a acuracia final (acc) e o tempo de treinamento (time) encontrado no seu experimento pra cada dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEUK1xk6Fk48"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"mnist\": {\"time\": 181, \"acc\": 0.9930},\n",
        "    \"fashion_mnist\": {\"time\": 244, \"acc\": 0.9168},\n",
        "    \"cifar10\": {\"time\": 545, \"acc\": 0.7495},\n",
        "    \"cifar100\": {\"time\": 600, \"acc\": 0.3581},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo \"Caro\".\n",
        "\n",
        "**NÃO TENTAR TREINAR NO COLLAB NORMAL, O GOOGLE VAI BLOQUEAR OS RECURSOS.**"
      ],
      "metadata": {
        "id": "5Pg20utltEau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_expensive_network():\n",
        "  model =  keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(128, (1, 1), strides=1, activation='relu', input_shape=input_shape),\n",
        "      Residual(128, (7,7), strides=1),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "      tf.keras.layers.Conv2D(128, (5, 5), strides=1, activation='relu'),\n",
        "      #tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "      tf.keras.layers.Conv2D(128, (3, 3), strides=1, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.GlobalAveragePooling2D()\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "1Wxg7Bzarg8c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}